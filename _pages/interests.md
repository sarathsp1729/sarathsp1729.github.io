---
permalink: /interests/
title: "Interests"
excerpt: "Interests"
author_profile: true
redirect_from: 
  - /my_interests/
---
Understanding the comprehension of information in learning systems intrigues me. The effectiveness of neural networks in perception-like tasks, I believe, makes them the right candidate for exploring this horizon. In particular, the recent investigations towards understanding the generalization, shared weakness and blindspots of neural networks interest me. My thesis proposes a formulation of neural networks that lowers the empirical error and reduces the dependency on model selection as the train and test performance are comparable throughout the training profile. This is achieved by constraining each output of the neural network as a convex combination of the inputs; Input-Output Convex neural network (IOC-NN). Given that the pointwise maximum of convex functions can model functions of arbitrary complexity, despite the constraint, IOC-NN retains adequate capacity. We validate this empirically on multiple image classification benchmarks and published the results at ECML 2021.

Extending this idea, we observed that under these constraints, neural networks improve their test performance for MLP (non-geometrically optimized architecture). Experiments across datasets show that the constraint helps retrieve the network's performance, previously lost to overfitting. We also uncover other properties of IOC networks, like robustness to noise. In fact, IOC-NNs show robustness to fitting random labels like never seen before. I am keen to take forward this idea and explain the generalization of NN under IOC-like constraints. Exploring the explanations for the generalization and computing robust bounds for neural networks beyond PACS and uniform convergence intrigues me. I am further investigating the partition of input space created by the binary MoE of IOC-NNS. These partitions are mostly interpretable as a simple convex boundary can make reasonable predictions in them.

I am keen to explore topics spanning understanding the biases in NNs and improving their performance on natural adversarial and OOD samples. In recent years, there have been many attempts to tackle domain generalization using tailored methods. Validating the finding of a contemporary work, our work suggested using a straightforward empirical risk minimizer with a custom training protocol that outperforms SOTA results. We also evaluate the statistical significance of previous competitive methods on top of the proposed simple ERM. We also proposed a different setting for evaluation for dataset shift: a domain is kept out from each class for testing. To our surprise, despite seeing all domains during training, networks struggle to perform on this new benchmark. This shows that the challenge in learning lies elsewhere beyond traditional domain generalization. I would like to further study the limitations of neural models, such as the tendency to fall for shortcut learning, fitting spurious variances, and failing to perform on natural adversarial samples. I find recent literature that explores balancing the texture-shape bias in CNNs interesting. Exploring such deficits of learning systems and balancing their biases is an area I would love to pursue for my future research. 

Outside of my research areas, I am also interested in classical music. My taste for music spans from Bob Dylan to Nusrat Fateh Ali Khan(not that they are on the end of any spectrum!). I love reading and some of my favourite writers are Robert Sapolsky, Steven Pinker, Yuval Noah Harari and Christopher Hitchens. The books I read and the conversation I made and the work I do has greatly shaped how I see the world.
